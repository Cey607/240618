{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split  \n",
    "import re\n",
    "import random\n",
    "import jieba.posseg as pseg\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    bjsj                                               bjnr  \\\n",
      "0  2024-04-11 21:49:00.0  2024年4月11日 21时48分55秒 薛一铭( 180****5228 ，142427*...   \n",
      "1  2024-04-11 21:43:25.0  2024年4月11日 21时43分22秒 郭女士( 139****6828 ) 报警：沙XX...   \n",
      "2  2024-04-11 21:16:36.0  2024年4月11日 21时16分35秒 牛女士( 151****7579 ，142329*...   \n",
      "3  2024-04-11 21:09:29.0  2024年4月11日 21时9分28秒 王先生( 151****8799、140111***...   \n",
      "4  2024-04-11 21:01:51.0  2024年4月11日 10时44分48秒 黄志明( 151****3088 、350524*...   \n",
      "\n",
      "   bjlbdm  bjlxdm    bjxldm bjlbmc bjlxmc  bjxlmc  \n",
      "0      10  100100  100120.0   刑事案件     盗窃     NaN  \n",
      "1      10  100100  100199.0   刑事案件     盗窃     NaN  \n",
      "2      10  100100  100120.0   刑事案件     盗窃     NaN  \n",
      "3      10  100100  100120.0   刑事案件     盗窃     NaN  \n",
      "4      10  100100  100199.0   刑事案件     盗窃     NaN  \n",
      "处理后数据总数: 994\n"
     ]
    }
   ],
   "source": [
    "# 定义计算设备\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# Step 1: 数据加载\n",
    "data_path = 'data.xlsx'\n",
    "try:\n",
    "    df = pd.read_excel(data_path)\n",
    "    print(df.head())\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data from {data_path}: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Step 2: 数据预处理\n",
    "df = df[['bjnr', 'bjlbmc', 'bjlxmc']]\n",
    "df.columns = ['报警内容', '警情粗类', '警情细类']\n",
    "df.dropna(subset=['报警内容', '警情粗类', '警情细类'], inplace=True)\n",
    "\n",
    "# 正则表达式提取报警信息\n",
    "def extract_alarm_text(text):\n",
    "    match = re.search(r'报警：(.*)', text)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    else:\n",
    "        return text.strip()\n",
    "\n",
    "df['报警内容'] = df['报警内容'].apply(extract_alarm_text)\n",
    "\n",
    "# 同义词替换函数\n",
    "synonyms_dict = {\n",
    "    '报警': ['警报', '求助', '求援'],\n",
    "    # 可以根据实际需要添加更多的同义词对应关系\n",
    "}\n",
    "\n",
    "def synonym_replacement(text, n=1):\n",
    "    words = list(pseg.cut(text))  # 使用结巴分词来处理中文文本\n",
    "    new_words = []\n",
    "    replaced = False\n",
    "    for word, flag in words:\n",
    "        if word in synonyms_dict and len(synonyms_dict[word]) > 0:\n",
    "            synonym = random.choice(synonyms_dict[word])\n",
    "            new_words.append(synonym)\n",
    "            replaced = True\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    \n",
    "    # 如果没有替换成功，则随机替换一个词\n",
    "    if not replaced and len(words) > 0:\n",
    "        index = random.randint(0, len(words) - 1)\n",
    "        synonym = random.choice(synonyms_dict.get(words[index].word, [words[index].word]))  # 如果找不到同义词，则保持原词\n",
    "        new_words[index] = synonym\n",
    "    \n",
    "    return ''.join(new_words)\n",
    "\n",
    "# 数据增强：同义词替换\n",
    "augmented_data = []\n",
    "for _, row in df.iterrows():\n",
    "    augmented_data.append(row.to_dict())\n",
    "    augmented_content = synonym_replacement(row['报警内容'])\n",
    "    augmented_data.append({'报警内容': augmented_content, '警情粗类': row['警情粗类'], '警情细类': row['警情细类']})\n",
    "\n",
    "augmented_df = pd.DataFrame(augmented_data)\n",
    "\n",
    "# 剔除重复信息\n",
    "augmented_df.drop_duplicates(subset=['报警内容', '警情粗类', '警情细类'], keep='first', inplace=True)\n",
    "\n",
    "label_encoder_coarse = LabelEncoder()\n",
    "augmented_df['警情粗类编码'] = label_encoder_coarse.fit_transform(augmented_df['警情粗类'])\n",
    "\n",
    "label_encoder_fine = LabelEncoder()\n",
    "augmented_df['警情细类编码'] = label_encoder_fine.fit_transform(augmented_df['警情细类'])\n",
    "\n",
    "# 保存 LabelEncoder 对象\n",
    "with open('label_encoder_coarse.pkl', 'wb') as f:\n",
    "    pickle.dump(label_encoder_coarse, f)\n",
    "with open('label_encoder_fine.pkl', 'wb') as f:\n",
    "    pickle.dump(label_encoder_fine, f)\n",
    "\n",
    "# 打印处理后的数据信息\n",
    "print(f\"处理后数据总数: {len(augmented_df)}\")\n",
    "\n",
    "# 可选：保存处理后的数据\n",
    "augmented_df.to_excel('processed_data.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Step 3: 文本特征提取\u001b[39;00m\n\u001b[0;32m      2\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m BertTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m X_train_coarse, X_test_coarse, y_train_coarse, y_test_coarse \u001b[38;5;241m=\u001b[39m train_test_split(augmented_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m报警内容\u001b[39m\u001b[38;5;124m'\u001b[39m], augmented_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m警情粗类编码\u001b[39m\u001b[38;5;124m'\u001b[39m], test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m205392\u001b[39m)\n\u001b[0;32m      5\u001b[0m X_train_fine, X_test_fine, y_train_fine, y_test_fine \u001b[38;5;241m=\u001b[39m train_test_split(augmented_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m报警内容\u001b[39m\u001b[38;5;124m'\u001b[39m], augmented_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m警情细类编码\u001b[39m\u001b[38;5;124m'\u001b[39m], test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m205392\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode_data\u001b[39m(texts, labels, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "# Step 3: 文本特征提取\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "X_train_coarse, X_test_coarse, y_train_coarse, y_test_coarse = train_test_split(augmented_df['报警内容'], augmented_df['警情粗类编码'], test_size=0.2, random_state=205392)\n",
    "X_train_fine, X_test_fine, y_train_fine, y_test_fine = train_test_split(augmented_df['报警内容'], augmented_df['警情细类编码'], test_size=0.2, random_state=205392)\n",
    "\n",
    "def encode_data(texts, labels, max_length=128):\n",
    "    inputs = tokenizer(texts.tolist(), max_length=max_length, padding='max_length', truncation=True, return_tensors='pt')\n",
    "    inputs['labels'] = torch.tensor(labels.values)\n",
    "    return inputs\n",
    "\n",
    "train_coarse_encodings = encode_data(X_train_coarse, y_train_coarse)\n",
    "test_coarse_encodings = encode_data(X_test_coarse, y_test_coarse)\n",
    "train_fine_encodings = encode_data(X_train_fine, y_train_fine)\n",
    "test_fine_encodings = encode_data(X_test_fine, y_test_fine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "d:\\anaconda\\Lib\\site-packages\\accelerate\\accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fb3b97f035e4d1dba8b70aa1136e6ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3312, 'learning_rate': 3.9960000000000004e-05, 'epoch': 0.01}\n",
      "{'loss': 1.7974, 'learning_rate': 3.6e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee2879eb75d54c0ba7374d90c5505b61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5270096063613892, 'eval_runtime': 1.4467, 'eval_samples_per_second': 137.558, 'eval_steps_per_second': 17.281, 'epoch': 1.0}\n",
      "{'loss': 1.5574, 'learning_rate': 3.2000000000000005e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38b73b7f70494bbfa933874c5177cb4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4068944454193115, 'eval_runtime': 1.4928, 'eval_samples_per_second': 133.307, 'eval_steps_per_second': 16.747, 'epoch': 2.0}\n",
      "{'loss': 1.3393, 'learning_rate': 2.8e-05, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64344615043046829f3c33c3a70ab0af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2799484729766846, 'eval_runtime': 1.5786, 'eval_samples_per_second': 126.064, 'eval_steps_per_second': 15.837, 'epoch': 3.0}\n",
      "{'loss': 1.1074, 'learning_rate': 2.4e-05, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90b0832a48db4c9da47008107371bae6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2164697647094727, 'eval_runtime': 1.514, 'eval_samples_per_second': 131.444, 'eval_steps_per_second': 16.513, 'epoch': 4.0}\n",
      "{'loss': 0.8549, 'learning_rate': 2e-05, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "437c6694007e429d8bc2e6a8253d6ad8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0581930875778198, 'eval_runtime': 1.5989, 'eval_samples_per_second': 124.458, 'eval_steps_per_second': 15.635, 'epoch': 5.0}\n",
      "{'loss': 0.6389, 'learning_rate': 1.6000000000000003e-05, 'epoch': 6.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ffb91d2730148249f758e53184c305d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9233143925666809, 'eval_runtime': 1.5566, 'eval_samples_per_second': 127.847, 'eval_steps_per_second': 16.061, 'epoch': 6.0}\n",
      "{'loss': 0.4463, 'learning_rate': 1.2e-05, 'epoch': 7.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40b628f3b1a24f2f8e88310850414e33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.026775598526001, 'eval_runtime': 1.5287, 'eval_samples_per_second': 130.18, 'eval_steps_per_second': 16.354, 'epoch': 7.0}\n",
      "{'loss': 0.3355, 'learning_rate': 8.000000000000001e-06, 'epoch': 8.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "569d2dbee7a048febfff143bc787ff3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0527417659759521, 'eval_runtime': 1.5213, 'eval_samples_per_second': 130.808, 'eval_steps_per_second': 16.433, 'epoch': 8.0}\n",
      "{'loss': 0.2479, 'learning_rate': 4.000000000000001e-06, 'epoch': 9.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "413eda9389d14c91a7e350afdc0ee429",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.079898715019226, 'eval_runtime': 1.5117, 'eval_samples_per_second': 131.642, 'eval_steps_per_second': 16.538, 'epoch': 9.0}\n",
      "{'loss': 0.185, 'learning_rate': 0.0, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bece2f0587024781b4e304c4cb6b4c9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1008565425872803, 'eval_runtime': 1.5095, 'eval_samples_per_second': 131.827, 'eval_steps_per_second': 16.561, 'epoch': 10.0}\n",
      "{'train_runtime': 244.8695, 'train_samples_per_second': 32.507, 'train_steps_per_second': 4.084, 'train_loss': 0.8515378339290619, 'epoch': 10.0}\n"
     ]
    }
   ],
   "source": [
    "# Step 4: 模型训练\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: tensor[idx].to(device) for key, tensor in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "train_coarse_dataset = Dataset(train_coarse_encodings)\n",
    "test_coarse_dataset = Dataset(test_coarse_encodings)\n",
    "train_fine_dataset = Dataset(train_fine_encodings)\n",
    "test_fine_dataset = Dataset(test_fine_encodings)\n",
    "\n",
    "# 定义训练参数和保存检查点的参数\n",
    "training_args_coarse = TrainingArguments(\n",
    "    output_dir='./results_coarse',\n",
    "    num_train_epochs=10,  # 10轮训练\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    "    load_best_model_at_end=True,\n",
    "    logging_dir='./logs_coarse',  # 添加日志目录\n",
    "    logging_steps=100,  # 每100个步骤记录一次日志\n",
    "    logging_first_step=True,\n",
    "    save_steps=500,  # 每500个步骤保存一次检查点\n",
    "    learning_rate=4e-5,  # 设置学习率\n",
    ")\n",
    "\n",
    "# 定义Trainer对象来训练粗类分类模型\n",
    "model_coarse = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_encoder_coarse.classes_))\n",
    "model_coarse.to(device)\n",
    "\n",
    "trainer_coarse = Trainer(\n",
    "    model=model_coarse,\n",
    "    args=training_args_coarse,\n",
    "    train_dataset=train_coarse_dataset,\n",
    "    eval_dataset=test_coarse_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# 训练粗类分类模型\n",
    "trainer_coarse.train()\n",
    "\n",
    "# 保存粗类分类模型的最终检查点\n",
    "trainer_coarse.save_model('./checkpoint/coarse_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9e1de10bd4a4a409e9f11bfa41f563f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.0364, 'learning_rate': 4.996666666666667e-05, 'epoch': 0.01}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 32\u001b[0m\n\u001b[0;32m     23\u001b[0m trainer_fine \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     24\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel_fine,\n\u001b[0;32m     25\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args_fine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     28\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m     29\u001b[0m )\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# 训练细类分类模型\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m trainer_fine\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# 保存细类分类模型的最终检查点\u001b[39;00m\n\u001b[0;32m     35\u001b[0m trainer_fine\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./checkpoint/fine_model\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\transformers\\trainer.py:1555\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1553\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1554\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1555\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1556\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   1557\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[0;32m   1558\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[0;32m   1559\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[0;32m   1560\u001b[0m     )\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\transformers\\trainer.py:1842\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1836\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m   1837\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   1839\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1840\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1841\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m-> 1842\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1843\u001b[0m ):\n\u001b[0;32m   1844\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1845\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 定义细类分类模型的训练参数\n",
    "training_args_fine = TrainingArguments(\n",
    "    output_dir='./results_fine',\n",
    "    num_train_epochs=15, \n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    "    load_best_model_at_end=True,\n",
    "    logging_dir='./logs_fine',  # 添加日志目录\n",
    "    logging_steps=100,  # 每100个步骤记录一次日志\n",
    "    logging_first_step=True,\n",
    "    save_steps=500,  # 每500个步骤保存一次检查点\n",
    "    learning_rate=5e-5,  # 设置学习率\n",
    ")\n",
    "\n",
    "# 定义Trainer对象来训练细类分类模型\n",
    "model_fine = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_encoder_fine.classes_))\n",
    "model_fine.to(device)\n",
    "\n",
    "trainer_fine = Trainer(\n",
    "    model=model_fine,\n",
    "    args=training_args_fine,\n",
    "    train_dataset=train_fine_dataset,\n",
    "    eval_dataset=test_fine_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# 训练细类分类模型\n",
    "trainer_fine.train()\n",
    "\n",
    "# 保存细类分类模型的最终检查点\n",
    "trainer_fine.save_model('./checkpoint/fine_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c1d850b123f40409eaafb5b97ab6919",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'classification_report' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m preds_coarse \u001b[38;5;241m=\u001b[39m trainer_coarse\u001b[38;5;241m.\u001b[39mpredict(test_coarse_dataset)\n\u001b[0;32m      3\u001b[0m y_pred_coarse \u001b[38;5;241m=\u001b[39m preds_coarse\u001b[38;5;241m.\u001b[39mpredictions\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m粗类分类报告:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, classification_report(y_test_coarse, y_pred_coarse, labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(label_encoder_coarse\u001b[38;5;241m.\u001b[39mclasses_)), target_names\u001b[38;5;241m=\u001b[39mlabel_encoder_coarse\u001b[38;5;241m.\u001b[39mclasses_))\n\u001b[0;32m      6\u001b[0m preds_fine \u001b[38;5;241m=\u001b[39m trainer_fine\u001b[38;5;241m.\u001b[39mpredict(test_fine_dataset)\n\u001b[0;32m      7\u001b[0m y_pred_fine \u001b[38;5;241m=\u001b[39m preds_fine\u001b[38;5;241m.\u001b[39mpredictions\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'classification_report' is not defined"
     ]
    }
   ],
   "source": [
    "# Step 5: 模型评估\n",
    "preds_coarse = trainer_coarse.predict(test_coarse_dataset)\n",
    "y_pred_coarse = preds_coarse.predictions.argmax(-1)\n",
    "print(\"粗类分类报告:\\n\", classification_report(y_test_coarse, y_pred_coarse, labels=range(len(label_encoder_coarse.classes_)), target_names=label_encoder_coarse.classes_))\n",
    "\n",
    "preds_fine = trainer_fine.predict(test_fine_dataset)\n",
    "y_pred_fine = preds_fine.predictions.argmax(-1)\n",
    "print(\"细类分类报告:\\n\", classification_report(y_test_fine, y_pred_fine, labels=range(len(label_encoder_fine.classes_)), target_names=label_encoder_fine.classes_))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
