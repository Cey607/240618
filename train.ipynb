{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    bjsj                                               bjnr  \\\n",
      "0  2024-04-11 21:49:00.0  2024年4月11日 21时48分55秒 薛一铭( 180****5228 ，142427*...   \n",
      "1  2024-04-11 21:43:25.0  2024年4月11日 21时43分22秒 郭女士( 139****6828 ) 报警：沙XX...   \n",
      "2  2024-04-11 21:16:36.0  2024年4月11日 21时16分35秒 牛女士( 151****7579 ，142329*...   \n",
      "3  2024-04-11 21:09:29.0  2024年4月11日 21时9分28秒 王先生( 151****8799、140111***...   \n",
      "4  2024-04-11 21:01:51.0  2024年4月11日 10时44分48秒 黄志明( 151****3088 、350524*...   \n",
      "\n",
      "   bjlbdm  bjlxdm    bjxldm bjlbmc bjlxmc  bjxlmc  \n",
      "0      10  100100  100120.0   刑事案件     盗窃     NaN  \n",
      "1      10  100100  100199.0   刑事案件     盗窃     NaN  \n",
      "2      10  100100  100120.0   刑事案件     盗窃     NaN  \n",
      "3      10  100100  100120.0   刑事案件     盗窃     NaN  \n",
      "4      10  100100  100199.0   刑事案件     盗窃     NaN  \n",
      "处理后数据总数: 793\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# 检查GPU是否可用\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Step 1: 数据加载\n",
    "data_path = 'data.xlsx'\n",
    "df = pd.read_excel(data_path)\n",
    "print(df.head())\n",
    "\n",
    "# Step 2: 数据预处理\n",
    "df = df[['bjnr', 'bjlbmc', 'bjlxmc']]\n",
    "df.columns = ['报警内容', '警情粗类', '警情细类']\n",
    "df.dropna(subset=['报警内容', '警情粗类', '警情细类'], inplace=True)\n",
    "\n",
    "# 正则表达式提取报警信息\n",
    "def extract_alarm_text(text):\n",
    "    match = re.search(r'报警：(.*)', text)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    else:\n",
    "        return text.strip()\n",
    "\n",
    "df['报警内容'] = df['报警内容'].apply(extract_alarm_text)\n",
    "\n",
    "# 剔除重复信息\n",
    "df.drop_duplicates(subset=['报警内容', '警情粗类', '警情细类'], keep='first', inplace=True)\n",
    "\n",
    "label_encoder_coarse = LabelEncoder()\n",
    "df['警情粗类编码'] = label_encoder_coarse.fit_transform(df['警情粗类'])\n",
    "\n",
    "label_encoder_fine = LabelEncoder()\n",
    "df['警情细类编码'] = label_encoder_fine.fit_transform(df['警情细类'])\n",
    "\n",
    "import pickle\n",
    "\n",
    "# 保存 LabelEncoder 对象\n",
    "with open('label_encoder_coarse.pkl', 'wb') as f:\n",
    "    pickle.dump(label_encoder_coarse, f)\n",
    "with open('label_encoder_fine.pkl', 'wb') as f:\n",
    "    pickle.dump(label_encoder_fine, f)\n",
    "\n",
    "# 打印处理后的数据信息\n",
    "print(f\"处理后数据总数: {len(df)}\")\n",
    "\n",
    "# 可选：保存处理后的数据\n",
    "df.to_excel('processed_data.xlsx', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: 文本特征提取\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "X_train_coarse, X_test_coarse, y_train_coarse, y_test_coarse = train_test_split(df['报警内容'], df['警情粗类编码'], test_size=0.2, random_state=205392)\n",
    "X_train_fine, X_test_fine, y_train_fine, y_test_fine = train_test_split(df['报警内容'], df['警情细类编码'], test_size=0.2, random_state=205392)\n",
    "\n",
    "def encode_data(texts, labels, max_length=128):\n",
    "    inputs = tokenizer(texts.tolist(), max_length=max_length, padding='max_length', truncation=True, return_tensors='pt')\n",
    "    inputs['labels'] = torch.tensor(labels.values)\n",
    "    return inputs\n",
    "\n",
    "train_coarse_encodings = encode_data(X_train_coarse, y_train_coarse)\n",
    "test_coarse_encodings = encode_data(X_test_coarse, y_test_coarse)\n",
    "train_fine_encodings = encode_data(X_train_fine, y_train_fine)\n",
    "test_fine_encodings = encode_data(X_test_fine, y_test_fine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "d:\\anaconda\\Lib\\site-packages\\accelerate\\accelerator.py:444: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5eb2398ae7d43f6ba9ab810db9bc58e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1058, 'learning_rate': 3.995e-05, 'epoch': 0.01}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d0964f7f49847f89806df19647036bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.897748589515686, 'eval_runtime': 1.2668, 'eval_samples_per_second': 125.509, 'eval_steps_per_second': 15.787, 'epoch': 1.0}\n",
      "{'loss': 1.9071, 'learning_rate': 3.5000000000000004e-05, 'epoch': 1.25}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c395044ae302474caa8755c8525b02aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7327489852905273, 'eval_runtime': 1.285, 'eval_samples_per_second': 123.734, 'eval_steps_per_second': 15.564, 'epoch': 2.0}\n",
      "{'loss': 1.5944, 'learning_rate': 3.0000000000000004e-05, 'epoch': 2.5}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85110c0f609d48bfbb6c51e81101bcaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5316656827926636, 'eval_runtime': 1.311, 'eval_samples_per_second': 121.285, 'eval_steps_per_second': 15.256, 'epoch': 3.0}\n",
      "{'loss': 1.3797, 'learning_rate': 2.5e-05, 'epoch': 3.75}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adb98592a791486ab473b0e31a591a75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4068586826324463, 'eval_runtime': 1.3029, 'eval_samples_per_second': 122.038, 'eval_steps_per_second': 15.351, 'epoch': 4.0}\n",
      "{'loss': 1.031, 'learning_rate': 2e-05, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a3688373b0b44d8968f5d50ff3c6aa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4349815845489502, 'eval_runtime': 1.2744, 'eval_samples_per_second': 124.761, 'eval_steps_per_second': 15.693, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7181a4d3d0f409a98153b5790d66242",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3194950819015503, 'eval_runtime': 1.3157, 'eval_samples_per_second': 120.847, 'eval_steps_per_second': 15.201, 'epoch': 6.0}\n",
      "{'loss': 0.6914, 'learning_rate': 1.5000000000000002e-05, 'epoch': 6.25}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48fec0708f7e4ae09ec1b8b85176fe4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.38361656665802, 'eval_runtime': 1.3144, 'eval_samples_per_second': 120.965, 'eval_steps_per_second': 15.216, 'epoch': 7.0}\n",
      "{'loss': 0.5057, 'learning_rate': 1e-05, 'epoch': 7.5}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03f47fd6ee9a4c52bb1d9ff74d73aaf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.461667776107788, 'eval_runtime': 1.2564, 'eval_samples_per_second': 126.549, 'eval_steps_per_second': 15.918, 'epoch': 8.0}\n",
      "{'loss': 0.3472, 'learning_rate': 5e-06, 'epoch': 8.75}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d42fd3fb638645dfbbe67e574c90ae1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5069868564605713, 'eval_runtime': 1.2569, 'eval_samples_per_second': 126.504, 'eval_steps_per_second': 15.912, 'epoch': 9.0}\n",
      "{'loss': 0.2576, 'learning_rate': 0.0, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a085e960ce95414fbbf7ad98c0e87678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5308083295822144, 'eval_runtime': 1.2355, 'eval_samples_per_second': 128.698, 'eval_steps_per_second': 16.188, 'epoch': 10.0}\n",
      "{'train_runtime': 202.7584, 'train_samples_per_second': 31.269, 'train_steps_per_second': 3.946, 'train_loss': 0.9645163357257843, 'epoch': 10.0}\n"
     ]
    }
   ],
   "source": [
    "# Step 4: 模型训练\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: tensor[idx].to(device) for key, tensor in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "train_coarse_dataset = Dataset(train_coarse_encodings)\n",
    "test_coarse_dataset = Dataset(test_coarse_encodings)\n",
    "train_fine_dataset = Dataset(train_fine_encodings)\n",
    "test_fine_dataset = Dataset(test_fine_encodings)\n",
    "\n",
    "# 定义训练参数和保存检查点的参数\n",
    "training_args_coarse = TrainingArguments(\n",
    "    output_dir='./results_coarse',\n",
    "    num_train_epochs=10,  # 10轮训练\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    "    load_best_model_at_end=True,\n",
    "    logging_dir='./logs_coarse',  # 添加日志目录\n",
    "    logging_steps=100,  # 每100个步骤记录一次日志\n",
    "    logging_first_step=True,\n",
    "    save_steps=500,  # 每500个步骤保存一次检查点\n",
    "    learning_rate=4e-5,  # 设置学习率\n",
    ")\n",
    "\n",
    "# 定义Trainer对象来训练粗类分类模型\n",
    "model_coarse = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_encoder_coarse.classes_))\n",
    "model_coarse.to(device)\n",
    "\n",
    "trainer_coarse = Trainer(\n",
    "    model=model_coarse,\n",
    "    args=training_args_coarse,\n",
    "    train_dataset=train_coarse_dataset,\n",
    "    eval_dataset=test_coarse_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# 训练粗类分类模型\n",
    "trainer_coarse.train()\n",
    "\n",
    "# 保存粗类分类模型的最终检查点\n",
    "trainer_coarse.save_model('./checkpoint/coarse_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b5f8361dd9c4e08a9850f6715c19828",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1360 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.2207, 'learning_rate': 3.997058823529412e-05, 'epoch': 0.01}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e01218c061f4f7d8cbaf34bae88b4ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.9265012741088867, 'eval_runtime': 11.2258, 'eval_samples_per_second': 14.164, 'eval_steps_per_second': 1.782, 'epoch': 1.0}\n",
      "{'loss': 4.0159, 'learning_rate': 3.705882352941177e-05, 'epoch': 1.25}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9f2a0b586aa41eba1d956c3309279a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.8476178646087646, 'eval_runtime': 11.9078, 'eval_samples_per_second': 13.353, 'eval_steps_per_second': 1.68, 'epoch': 2.0}\n",
      "{'loss': 3.7703, 'learning_rate': 3.411764705882353e-05, 'epoch': 2.5}\n"
     ]
    }
   ],
   "source": [
    "# 定义细类分类模型的训练参数\n",
    "training_args_fine = TrainingArguments(\n",
    "    output_dir='./results_fine',\n",
    "    num_train_epochs=17, \n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    "    load_best_model_at_end=True,\n",
    "    logging_dir='./logs_fine',  # 添加日志目录\n",
    "    logging_steps=100,  # 每100个步骤记录一次日志\n",
    "    logging_first_step=True,\n",
    "    save_steps=500,  # 每500个步骤保存一次检查点\n",
    "    learning_rate=4e-5,  # 设置学习率\n",
    ")\n",
    "\n",
    "# 定义Trainer对象来训练细类分类模型\n",
    "model_fine = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_encoder_fine.classes_))\n",
    "model_fine.to(device)\n",
    "\n",
    "trainer_fine = Trainer(\n",
    "    model=model_fine,\n",
    "    args=training_args_fine,\n",
    "    train_dataset=train_fine_dataset,\n",
    "    eval_dataset=test_fine_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# 训练细类分类模型\n",
    "trainer_fine.train()\n",
    "\n",
    "# 保存细类分类模型的最终检查点\n",
    "trainer_fine.save_model('./checkpoint/fine_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: 模型评估\n",
    "preds_coarse = trainer_coarse.predict(test_coarse_dataset)\n",
    "y_pred_coarse = preds_coarse.predictions.argmax(-1)\n",
    "print(\"粗类分类报告:\\n\", classification_report(y_test_coarse, y_pred_coarse, labels=range(len(label_encoder_coarse.classes_)), target_names=label_encoder_coarse.classes_))\n",
    "\n",
    "preds_fine = trainer_fine.predict(test_fine_dataset)\n",
    "y_pred_fine = preds_fine.predictions.argmax(-1)\n",
    "print(\"细类分类报告:\\n\", classification_report(y_test_fine, y_pred_fine, labels=range(len(label_encoder_fine.classes_)), target_names=label_encoder_fine.classes_))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
